# Prebloom Full Stack Docker Compose
# 
# Usage:
#   docker compose -f docker-compose.prebloom.yml up --build
#
# Access:
#   Frontend: http://localhost:8080
#   API: http://localhost:8080/prebloom/
#
# Environment variables (create .env file):
#   ANTHROPIC_API_KEY=sk-ant-...

services:
  # Ollama: Local LLM for text cleanup (Llama 3.2 3B)
  ollama:
    image: ollama/ollama:latest
    expose:
      - "11434"
    volumes:
      - ollama-models:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Whisper: Local speech-to-text service
  whisper:
    build:
      context: ./whisper-service
      dockerfile: Dockerfile
    environment:
      WHISPER_MODEL: ${WHISPER_MODEL:-medium}
    expose:
      - "8000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Model loading takes time

  # Backend: Bloem + Prebloom API
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      HOME: /home/node
      NODE_ENV: production
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      GEMINI_API_KEY: ${GEMINI_API_KEY}
      SUPABASE_URL: ${SUPABASE_URL}
      SUPABASE_SERVICE_ROLE_KEY: ${SUPABASE_SERVICE_ROLE_KEY}
      CLAWDBOT_GATEWAY_TOKEN: ${CLAWDBOT_GATEWAY_TOKEN:-prebloom-internal}
      CLAWDBOT_GATEWAY_BIND: lan
      WHISPER_SERVICE_URL: http://whisper:8000
      OLLAMA_URL: http://ollama:11434
      CLEANUP_MODEL: ${CLEANUP_MODEL:-llama3.2:3b}
      USE_LOCAL_CLEANUP: ${USE_LOCAL_CLEANUP:-false}
    depends_on:
      whisper:
        condition: service_healthy
      ollama:
        condition: service_healthy
    volumes:
      - prebloom-config:/home/node/.clawdbot
    expose:
      - "3001"
    init: true
    restart: unless-stopped
    command:
      [
        "node",
        "dist/index.js",
        "gateway",
        "--bind",
        "lan",
        "--port",
        "3001",
        "--allow-unconfigured"
      ]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:3001/prebloom/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Frontend: React + Nginx
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "${PREBLOOM_PORT:-8080}:80"
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  prebloom-config:
  ollama-models:
